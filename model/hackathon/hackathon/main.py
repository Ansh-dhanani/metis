#!/usr/bin/env python3
"""
METIS-CORE CLI

Command-line interface for evaluating candidates.
Outputs strict JSON for model communication and Markdown for HR reports.
"""

import argparse
import json
import sys
from datetime import datetime
from pathlib import Path

from rich.console import Console
from rich.panel import Panel
from rich.table import Table

from metis.evaluator import evaluate_candidate
from metis.resume_parser import read_resume_file


console = Console()

# Default output directory
DEFAULT_OUTPUT_DIR = Path("evaluations")


def get_timestamp() -> str:
    """Get timestamp for unique filenames."""
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def read_file(path: str) -> str:
    """Read text from a file (supports .txt and .pdf)."""
    try:
        if not Path(path).exists():
            console.print(f"[red]Error: File not found: {path}[/red]")
            sys.exit(1)
        return read_resume_file(str(path))
    except Exception as e:
        console.print(f"[red]Error reading file {path}: {e}[/red]")
        sys.exit(1)


def generate_hr_report(result: dict, candidate_name: str = "Candidate") -> str:
    """Generate HR-readable Markdown report."""
    score = result.get("overall_score", 0)
    confidence = result.get("confidence_level", "low")
    section_scores = result.get("section_scores", {})
    
    # Determine assessment level
    if score >= 75:
        assessment = "STRONG CANDIDATE"
        badge = "[PASS]"
    elif score >= 55:
        assessment = "QUALIFIED CANDIDATE" 
        badge = "[REVIEW]"
    elif score >= 40:
        assessment = "BELOW AVERAGE"
        badge = "[CAUTION]"
    else:
        assessment = "WEAK CANDIDATE"
        badge = "[REJECT]"
    
    report = f"""# METIS-CORE Evaluation Report

**Candidate:** {candidate_name}  
**Date:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**Model:** {result.get("model", "metis_core_v1")}

---

## {badge} Overall Score: {score}/100

**Assessment:** {assessment}  
**Confidence:** {confidence.upper()}

---

## Section Breakdown

| Category | Score | Max | Status |
|----------|-------|-----|--------|
| Skill Evidence | {section_scores.get("skill_evidence", 0)} | 30 | {"PASS" if section_scores.get("skill_evidence", 0) >= 21 else "WARN" if section_scores.get("skill_evidence", 0) >= 12 else "FAIL"} |
| Project Authenticity | {section_scores.get("project_authenticity", 0)} | 25 | {"PASS" if section_scores.get("project_authenticity", 0) >= 18 else "WARN" if section_scores.get("project_authenticity", 0) >= 10 else "FAIL"} |
| Professional Signals | {section_scores.get("professional_signals", 0)} | 15 | {"PASS" if section_scores.get("professional_signals", 0) >= 11 else "WARN" if section_scores.get("professional_signals", 0) >= 6 else "FAIL"} |
| Impact & Outcomes | {section_scores.get("impact_outcomes", 0)} | 15 | {"PASS" if section_scores.get("impact_outcomes", 0) >= 11 else "WARN" if section_scores.get("impact_outcomes", 0) >= 6 else "FAIL"} |
| Resume Integrity | {section_scores.get("resume_integrity", 0)} | 15 | {"PASS" if section_scores.get("resume_integrity", 0) >= 12 else "WARN" if section_scores.get("resume_integrity", 0) >= 8 else "FAIL"} |

---

## Strength Signals

"""
    
    strengths = result.get("strength_signals", [])
    if strengths:
        for s in strengths:
            report += f"- [+] {s}\n"
    else:
        report += "- No significant strengths identified\n"
    
    report += "\n---\n\n## Risk Signals\n\n"
    
    risks = result.get("risk_signals", [])
    if risks:
        for r in risks:
            report += f"- [!] {r}\n"
    else:
        report += "- No significant risks identified\n"
    
    ats_flags = result.get("ats_flags", [])
    if ats_flags:
        report += "\n---\n\n## ATS Flags\n\n"
        for f in ats_flags:
            report += f"- [FLAG] {f}\n"
    
    report += f"""
---

## HR Summary

{result.get("final_reasoning", "No summary available.")}

---

*Generated by METIS-CORE v1 | Confidence: {confidence} | {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*
"""
    
    return report


def save_output(result: dict, output_path: Path, format_type: str, candidate_name: str = "Candidate"):
    """Save evaluation output to file."""
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    if format_type == "json":
        content = json.dumps(result, indent=2, ensure_ascii=False)
    else:  # markdown
        content = generate_hr_report(result, candidate_name)
    
    output_path.write_text(content, encoding="utf-8")
    return output_path


def display_result(result: dict, output_format: str = "rich"):
    """Display evaluation result to console."""
    
    if output_format == "json":
        print(json.dumps(result, indent=2, ensure_ascii=False))
        return
    
    # Rich formatted output
    score = result.get("overall_score", 0)
    confidence = result.get("confidence_level", "low")
    
    if score >= 70:
        score_color = "green"
    elif score >= 50:
        score_color = "yellow"
    else:
        score_color = "red"
    
    console.print(Panel.fit(
        f"[bold {score_color}]METIS-CORE Evaluation (v1)[/bold {score_color}]",
        border_style=score_color
    ))
    
    console.print(f"\n[bold]Overall Score:[/bold] [{score_color}]{score}/100[/{score_color}]")
    console.print(f"[bold]Confidence:[/bold] {confidence}")
    
    section_scores = result.get("section_scores", {})
    table = Table(title="Section Scores", show_header=True)
    table.add_column("Category", style="cyan")
    table.add_column("Score", justify="right")
    table.add_column("Max", justify="right")
    
    sections = [
        ("Skill Evidence", section_scores.get("skill_evidence", 0), 30),
        ("Project Authenticity", section_scores.get("project_authenticity", 0), 25),
        ("Professional Signals", section_scores.get("professional_signals", 0), 15),
        ("Impact & Outcomes", section_scores.get("impact_outcomes", 0), 15),
        ("Resume Integrity", section_scores.get("resume_integrity", 0), 15),
    ]
    
    for name, score_val, max_val in sections:
        ratio = score_val / max_val if max_val > 0 else 0
        if ratio >= 0.7:
            style = "green"
        elif ratio >= 0.4:
            style = "yellow"
        else:
            style = "red"
        table.add_row(name, f"[{style}]{score_val}[/{style}]", str(max_val))
    
    console.print(table)
    
    strengths = result.get("strength_signals", [])
    if strengths:
        console.print("\n[bold green]+ Strength Signals:[/bold green]")
        for s in strengths:
            console.print(f"  - {s}")
    
    risks = result.get("risk_signals", [])
    if risks:
        console.print("\n[bold red]! Risk Signals:[/bold red]")
        for r in risks:
            console.print(f"  - {r}")
    
    ats_flags = result.get("ats_flags", [])
    if ats_flags:
        console.print("\n[bold yellow]* ATS Flags:[/bold yellow]")
        for f in ats_flags:
            console.print(f"  - {f}")
    
    reasoning = result.get("final_reasoning", "")
    if reasoning:
        console.print(f"\n[bold]HR Summary:[/bold] {reasoning}")


def main():
    parser = argparse.ArgumentParser(
        description="METIS-CORE: Resume Signal Evaluator",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Console output only
  python main.py --resume resume.txt
  
  # Auto-save with timestamps (recommended)
  python main.py --resume resume.txt --save --name "John Doe"
  # Creates: evaluations/john_doe_20260207_120000.json
  #          evaluations/john_doe_20260207_120000.md
  
  # Custom output paths
  python main.py --resume resume.txt --output-json custom.json --output-md custom.md
  
  # With GitHub verification
  python main.py --resume resume.txt --github https://github.com/user --save
        """
    )
    
    parser.add_argument(
        "--resume",
        required=True,
        help="Path to resume text file"
    )
    
    parser.add_argument(
        "--github",
        help="GitHub profile URL (optional)"
    )
    
    parser.add_argument(
        "--portfolio",
        help="Portfolio website URL (optional)"
    )
    
    parser.add_argument(
        "--name",
        default="candidate",
        help="Candidate name for reports"
    )
    
    parser.add_argument(
        "--save",
        action="store_true",
        help="Auto-save JSON and Markdown with timestamped filenames"
    )
    
    parser.add_argument(
        "--output-dir",
        default="evaluations",
        help="Output directory for saved files (default: evaluations)"
    )
    
    parser.add_argument(
        "--output-json",
        help="Custom path for JSON output"
    )
    
    parser.add_argument(
        "--output-md",
        help="Custom path for Markdown HR report"
    )
    
    parser.add_argument(
        "--format",
        choices=["rich", "json"],
        default="rich",
        help="Console output format (default: rich)"
    )
    
    parser.add_argument(
        "--quiet", "-q",
        action="store_true",
        help="Suppress console output"
    )
    
    args = parser.parse_args()
    
    # Read resume
    resume_text = read_file(args.resume)
    
    if not args.quiet and args.format != "json":
        console.print("[dim]Running METIS-CORE evaluation...[/dim]\n")
    
    try:
        result = evaluate_candidate(
            resume_text=resume_text,
            github_url=args.github,
            portfolio_url=args.portfolio,
        )
        
        # Generate timestamped filename base
        timestamp = get_timestamp()
        safe_name = args.name.lower().replace(" ", "_").replace("-", "_")
        base_filename = f"{safe_name}_{timestamp}"
        output_dir = Path(args.output_dir)
        
        # Auto-save if --save flag is set
        if args.save:
            json_path = output_dir / f"{base_filename}.json"
            md_path = output_dir / f"{base_filename}.md"
            
            save_output(result, json_path, "json")
            save_output(result, md_path, "markdown", args.name)
            
            if not args.quiet:
                console.print(f"[green]JSON saved:[/green] {json_path}")
                console.print(f"[green]HR Report saved:[/green] {md_path}")
        
        # Custom JSON output
        if args.output_json:
            json_path = Path(args.output_json)
            save_output(result, json_path, "json")
            if not args.quiet:
                console.print(f"[green]JSON saved:[/green] {json_path}")
        
        # Custom Markdown output
        if args.output_md:
            md_path = Path(args.output_md)
            save_output(result, md_path, "markdown", args.name)
            if not args.quiet:
                console.print(f"[green]HR Report saved:[/green] {md_path}")
        
        # Console output
        if not args.quiet:
            if args.save or args.output_json or args.output_md:
                console.print()  # Blank line after save messages
            display_result(result, args.format)
        elif args.format == "json" and not args.output_json and not args.save:
            # If quiet but no file output, still print JSON to stdout
            print(json.dumps(result, indent=2, ensure_ascii=False))
        
    except Exception as e:
        error_result = {
            "model": "metis_core_v1",
            "overall_score": 0,
            "section_scores": {},
            "strength_signals": [],
            "risk_signals": [f"Evaluation error: {str(e)}"],
            "ats_flags": [],
            "confidence_level": "low",
            "final_reasoning": "Evaluation failed due to error."
        }
        
        if args.format == "json" or args.output_json:
            print(json.dumps(error_result, indent=2))
        else:
            console.print(f"[bold red]Evaluation failed:[/bold red] {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()
